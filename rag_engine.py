import os
import time
from langchain_groq import ChatGroq
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class RAGEngine:
    def __init__(self):
        print("---------- ðŸš€ å¼•æ“Žåˆå§‹åŒ–å¼€å§‹ ----------")
        
        # 1. Initialize LLM (Groq)
        print("1ï¸âƒ£ è¿žæŽ¥ Groq API...", end=" ", flush=True)
        self.llm = ChatGroq(
            model="llama-3.3-70b-versatile", 
            temperature=0.1
        )
        print("âœ… å®Œæˆ")
        
        # 2. Initialize Embeddings
        print("2ï¸âƒ£ åŠ è½½ Embedding æ¨¡åž‹ (é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½ ~100MB)...")
        start_time = time.time()
        
        # è¿™é‡Œæ˜¯å¯èƒ½å¡ä½çš„åœ°æ–¹
        self.embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        
        duration = time.time() - start_time
        print(f"âœ… Embedding æ¨¡åž‹åŠ è½½å®Œæˆ! è€—æ—¶: {duration:.2f} ç§’")
        
        # 3. Vector Store Path
        self.persist_directory = "./chroma_db"
        print("---------- ðŸ å¼•æ“Žåˆå§‹åŒ–ç»“æŸ ----------")
        
    def process_pdf(self, file_path):
        """Load PDF -> Chunk text -> Store in Vector DB"""
        print(f"ðŸ“„ æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")
        
        # A. Load PDF
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        print(f"   - è¯»å–åˆ° {len(docs)} é¡µ")
        
        # B. Split Text
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, 
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(docs)
        print(f"   - åˆ‡åˆ†ä¸º {len(splits)} ä¸ªæ–‡æœ¬å—")
        
        # C. Store in ChromaDB
        print("   - æ­£åœ¨å†™å…¥å‘é‡æ•°æ®åº“ (ChromaDB)...", end=" ", flush=True)
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        print("âœ… å†™å…¥å®Œæˆ")
        
        return vectorstore

    def chat(self, query, vectorstore):
        """Retrieve context -> Generate Answer"""
        
        # 1. Define Retriever
        # Search for top 3 most relevant chunks
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        
        # 2. Define Prompt
        # Standard RAG prompt structure
        prompt = ChatPromptTemplate.from_template("""
        You are a helpful assistant for question-answering tasks.
        Use the following pieces of retrieved context to answer the question.
        If the answer is not in the context, just say that you don't know. 
        Don't try to make up an answer.
        
        <context>
        {context}
        </context>
        
        Question: {input}
        """)
        
        # 3. Build Chain
        document_chain = create_stuff_documents_chain(self.llm, prompt)
        retrieval_chain = create_retrieval_chain(retriever, document_chain)
        
        # 4. Execute
        response = retrieval_chain.invoke({"input": query})
        
        return response